---
title: "Additional Information/Sensitivity Analyses"
subtitle: "_Online Supplement A to Staton et al._"
output:
  bookdown::html_document2:
    theme: cerulean
    toc: true
    toc_float: true
    toc_depth: 2
    number_sections: false
bibliography: cites.bib
link-citations: yes
csl: citation-style.csl
editor_options: 
  chunk_output_type: console
---

**NOTE:** All figure and table references apply only to this supplement, not to the the main text or other supplements, unless explicitly stated.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, fig.align = "center")

# directory to knit from: the project root, which is two directories up from here
knitr::opts_knit$set(root.dir = "../../")
```

```{r packages, message = F, warning = F}
source("load-packages.R")
```

# Section #1: Imputation of Missing Length-at-Age data

For mean length-at-age data (mid-eye to tail-fork; METF, mm), we used individual fish records sampled at weirs distributed throughout the Kuskokwim drainage dating back to 1976. These data were used for two purposes in our analysis: 

(1)  To inform the relative reproductive output of females of different ages (i.e., $z_{t,a,s}$), by inserting the METF values from the Kuskokwim into fecundity/ovary mass versus METF relationships obtained from other systems. 
(2)  To inform the relative selectivity of the fisheries using different mesh sizes via the Pearson gillnet selectivity function.

Both uses required that a value be present for each year, age, and sex. However, there were cases where no fish of a given age/sex class were sampled at any of the weirs operating throughout the basin. To impute values in these cases, we used a linear interpolation rule for missing values between two observations. If the first or last year in the data set was missing, we used the average of the most-recent 5 or 10 years, whichever was lesser that still provided a non-missing value.

The output of this rule is shown in Figure 1.1, below. In general, most interpolation occurred (a) in the rare age/sex classes (age-4 females, age-7 of both sexes) and (b) at the beginning of the time series when fewer weirs were operational. No weirs operated in 1998, which explains the missing value for all ages/sexes that year.

**Figure 1.1**: Time series (1976 -- 2019) of escapement average METF by age class including observed (blue) and imputed (red) values. Vertical lines represent the break points of the time series split into thirds (~14 years each); the percentages denote the percent of all values that were imputed for that time block by age/sex.

```{r workspace1, fig.width = 5, fig.height = 7}
dat = read.csv("1-data-prep/b-length-data/outputs/esc-mean-length-no-interp.csv")
dat_all = read.csv("1-data-prep/b-length-data/outputs/esc-mean-length.csv")

years = dat[,"year"]

dat = dat[,-1]
dat_all = dat_all[,-1]

ages = paste("Age", 4:7)

A = c(paste(ages, "Females"), paste(ages, "Males"))
breaks = c(rep(1, 15), rep(2, 14), rep(3, 15))
mids = tapply(years, breaks, mean)

line1 = mean(years[c(max(which(breaks == 1)), min(which(breaks == 2)))])
line2 = mean(years[c(max(which(breaks == 2)), min(which(breaks == 3)))])


par(mfcol = c(4,2), mar = c(0.5, 0.5, 2, 0.5), oma = c(3,4,0,2), tcl = -0.25, mgp = c(2,0.5,0))
for (a in 1:8) {
  plot(dat[,a] ~ years, type = "n", xaxt = "n", yaxt = "n", main = A[a],
       ylim = range(dat_all[,a]) * c(1, 1.05)
       # ylim = c(500, 1200)
       )
  abline(v = c(line1, line2), col = "grey")
  points(dat_all[,a] ~ years, pch = 16, col = "tomato")
  points(dat[,a] ~ years, pch = 16, col = "skyblue")
  p_missing = tapply(dat[,a], breaks, function(x) paste0(round(sum(is.na(x))/length(x), 2) * 100, "%"))
  usr = par("usr"); ydiff = diff(usr[3:4])
  text(x = mids, y = rep(usr[4] - ydiff * 0.075, 3), labels = p_missing)
  abline(h = usr[4] - ydiff * 0.15, col = "grey")
  axis(side = 1, labels = ifelse(a %in% c(4, 8), T, F))
  axis(side = ifelse(a <= 4, 2, 4), las = 2)
  box()
}
mtext(side = 1, outer = T, line = 1.5, "Year of Observation")
mtext(side = 2, outer = T, line = 2.5, "Average Escapement METF (mm)")

```

# Section #2: Alternative Length-Fecundity Relationships

```{r workspace2}
model = 1  # just needed to build the data
source("2-model-fit/1-compile-data.R")
source("load-functions.R")
rm(model) # clear out the model object 

out_dir = "model-output/permanent"
out_files = dir(out_dir, full.names = T)

# select the models to read in models

# main text E-0 and E-ASL models
keep_mods = c(3, 10)

# models E-0 and E-ASL for different fecundity vs. length data sets
altered_egg_mods = mod_key$model[mod_key$supplementary == 2]

keep_mods = c(keep_mods, altered_egg_mods)

keep_mods = paste(paste0("-", keep_mods, "\\."), collapse = "|")
out_files = out_files[str_detect(out_files, keep_mods)]

# the file names of the output files
postfiles = out_files[str_detect(out_files, "post")]
mods = str_extract(postfiles, "[0-9]+")
metafiles = out_files[str_detect(out_files, "meta")]
msyfiles = out_files[str_detect(out_files, "msy")]

# create empty objects to store the output from each model
meta = list()
post_list = list()
msy = NULL

# read in the posterior samples and meta data
for (i in 1:length(mods)) {
  post_list[[i]] = readRDS(postfiles[i])
  meta[[i]] = readRDS(metafiles[i])
}

# create the ids for each model
ids = unlist(lapply(meta, id_model, unit = F, trends = T, src = T, rand_age = F, ess = F))

# read in the msy equilibrium quantities
msy = readRDS(msyfiles[1])
for (i in 2:length(mods)) {
  msy = abind(msy, readRDS(msyfiles[i]), along = 5)
}

# give the objects model identifiers
names(meta) = ids
names(post_list) = ids
dimnames(msy)[[5]] = ids
```

```{r model-identifers2}

# reorder the models
ordered = c(
  "0-YK_eagle",
  "0-Kusko",
  "0-Kenai",
  "0-Ukleet",
  "0-YK_tan",     
  "0-YK_taktat",
  "0-YK_56",
  
  "ASL-YK_eagle",
  "ASL-Kusko",
  "ASL-Kenai",
  "ASL-Ukleet",
  "ASL-YK_tan",
  "ASL-YK_taktat",
  "ASL-YK_56"
)
msy = msy[,,,,ordered]
post_list = post_list[ordered]
meta = meta[ordered]
ids = ordered

col = character(length(ids))
col[str_detect(ids, "0")] = "skyblue"
col[str_detect(ids, "ASL")] = "tomato"
lty = rep(1:4, 2)
lwd = c(3, rep(1, length(ids) - 1))
```

## Motivation and Methods

The conclusions from our analysis may depend on the shape of the assumed relationship between female spawner size (METF) and reproductive output (e.g., fecundity). As described in the main-text, we deemed the length-fecundity data set available for the Kuskokwim River Chinook salmon population to be unreliable, so we used a relationship obtained from the Yukon River Chinook salmon population, sampled at Eagle, AK in the years 2008 -- 2010 to inform our primary analysis. However, we wished to evaluate whether our conclusions would have changed had we used other assumed relationships. Table \@ref(tab:table1) below shows the sources of 6 additional length-fecundity data sets we found for Chinook salmon populations in Alaska:

```{r table1}

mods = unlist(lapply(meta[1:7], function(m) m$model))

coefs = mod_key[mod_key$model %in% mods,c("a_coef", "b_coef")]

srcs = c(
  "YK_eagle" = "@ohlberger-etal-2020",
  "Kusko" = "Jim Boersma, unpublished data",
   "Kenai" = "@fleischman-reimer-2017",
   "Ukleet" = "@bell-kent-2012",
   "YK_tan" = "@skaugstad-mccracken-1991",
   "YK_taktat" = "Joel Harding, unpublished data",
   "YK_56" = "@jasper-evenson-2006"
)
systems = c(
  "YK_eagle" = "Yukon River (Eagle; CAN Origin)",
  "Kusko" = "Kuskokwim",
   "Kenai" = "Kenai River",
   "Ukleet" = "Unalakleet",
   "YK_tan" = "Yukon River (Tanana)",
   "YK_taktat" = "Yukon River (Takhini/Tatchun; CAN Origin)",
   "YK_56" = "Yukon River Districts Y5/Y6"
)
years = c(
  "YK_eagle" = "2008 -- 2010",
  "Kusko" = "2009 -- 2018",
   "Kenai" = "1989",
   "Ukleet" = "2008 -- 2010",
   "YK_tan" = "1989",
   "YK_taktat" = "1990 -- 2017",
   "YK_56" = "2005"
)

meta_tab = read.csv("3-post-process/os-sensitivity/fecund-meta.csv", stringsAsFactors = F)
tab = data.frame(
  abbrev = names(systems),
  dataset = letters[1:7],
  system = systems,
  years = years,
  a = coefs$a_coef,
  b = coefs$b_coef,
  source = srcs,
  stringsAsFactors = F
)

tab = merge(tab, meta_tab, by = "abbrev", sort = F)
tab$cv = paste0(round(tab$sd/tab$mean * 100), "%")

tab$mean_cv = paste0(round(tab$mean), " (", tab$cv, ")")
tab$range = paste0(tab$min, " -- ", tab$max)

tab = tab[,c("dataset", "system", "a", "b", "years", "nobs", "mean_cv", "range", "source")]

colnames(tab) = c("Data Set", "System", "a<sup>&dagger;</sup>", "b<sup>&dagger;</sup>", "Range of Years<sup>&Dagger;</sup>", "# Sampled", "Mean METF mm (%CV)", "METF mm Range", "Source")
rownames(tab) = NULL
tab$`Data Set`[1] = "a<sup>*</sup>"
tab %>%
  kable(align = "l", escape = F, caption = "Chinook salmon length-fecundity data sets used in this sensitivity analysis.") %>%
  kable_styling(full_width = F, bootstrap_options = c("condensed")) %>%
  add_header_above(c(" " = 2, "Power Function" = 2, "Sample Summary" = 4, " " = 1)) %>%
  footnote(symbol = c(
    "This is the data set used for all analyses in the main text.",
    "Coefficients of the power function: $a \\cdot \\mathrm{METF}^b$",
    "Not necessarily every year in range was sampled."),
    escape = F)

```

For this sensitivity analysis, we recalculated $z_{t,a,s}$ (mean reproductive output per spawner of age $a$ and sex $s$ in calendar year $t$) using the power function coefficients shown in Table \@ref(tab:table1). For each of the 7 data sets (6 additional to the main-text analysis), we fitted model E-0 (no trends allowed for age-, sex-, or length-at-return) and model E-ASL (all trends allowed). We then compared the output to that obtained when using data set **a**.

### Results

There was some variability in the expected fecundity at size among the different data sets we used, both in a absolute sense (Figure 2.1a) and a relative sense (Figure 2.1b). When standardized to the maximum point for each curve, the relative output of different sized females showed variable increases with fish size among data sources (Figure 2.1b). This is a result of the different values of the $b$ coefficient as shown in Table \@ref(tab:table1): steeper increases are associated with higher $b$ coefficients. Because of the vertical difference in the absolute relationships (Figure 2.1a), the expected fecundity per female spawner varied between model fits, requiring that the $\alpha$ parameter to scale accordingly to maintain similar recruitment time series (Figure 2.2a). We found that the value of the $\alpha$ parameter (representing maximum adults produced per egg) was inversely related to the scale of fecundity (Figure 2.2b). Because the spawner-recruit parameters could scale to fecundity, the estimated values and temporal trends of $S_{\mathrm{MSC}}$ were essentially identical among the model fits using different assumed fecundity relationships (Figure 2.3a,b).

#### Fecundity Relationships {.tabset .tabset-pills .tabset-fade}

**Figure 2.1:** Relationship between female length (METF in mm) and fecundity, obtained using the coefficients presented in Table \@ref(tab:table1)). (a) raw fecundity values, (b) fecundity scaled to that produced by the largest mean length-at-age value in the Kuskokwim River data set.

##### (a) Raw

```{r}

x = mod_key[mod_key$model %in% mods & mod_key$age_trend == 0, c("source", "src_abbrev", "a_coef", "b_coef")]
mesh8_perim = 8 * 2* 25.4  
f_len = rlm[,,1,1] * mesh8_perim
l_pred = seq(min(f_len), max(f_len), length = 30)
out = apply(x[,3:4], 1, function(i) i["a_coef"] * l_pred ^i["b_coef"])

srcs = letters[1:7]

par(mar = c(3,3,1,1), mgp = c(2,0.5,0), tcl = -0.25, xaxs = "i")
matplot(x = l_pred, y = out, type = "l", lwd = c(3,1,1,1,1,1,1), lty = 1, col = "black",
        xlim = c(500, 1010), xaxt = "n", xlab = "METF (mm)", ylab = "Predicted Fecundity")
text(x = 980, y = out[30,], labels = srcs, cex = 0.9, font = c(2,1,1,1,1,1,1),
     pos = 4)
axis(side = 1, at = seq(500, 1000, 100))
```

##### (b) Standardized

```{r}
out2 = apply(out, 2, function(x) x/max(x))
par(mar = c(3,3,1,1), mgp = c(2,0.5,0), tcl = -0.25, xaxs = "i")
matplot(x = l_pred, y = out2, type = "l", lwd = c(3,1,1,1,1,1,1), lty = 1,
        col = c("black"),
        xlim = c(500, 1000), xaxt = "n", xlab = "METF (mm)",
        ylab = "Predicted Fecundity (Scaled to max. METF)")
text(x = 520, y = out2[1,], labels = srcs, cex = 0.9, font = c(2,1,1,1,1,1,1),
     pos = 2)
axis(side = 1, at = seq(500, 1000, 100))
```

#### Estimates of $\alpha$ {.tabset .tabset-pills .tabset-fade}

**Figure 2.2:** (a) The posterior distribution of the $\alpha$ parameter estimated from model E-0 under each fecundity data set (median with thick and thin error bars representing the 50% and 95% credible regions, respectively), in arbitrary order (that presented in Table \@ref(tab:table1). (b) Same as (a), but with $\alpha$ plotted against the the expected fecundity of a female with METF of 800 mm from each data set.

##### (a)

```{r}
power_f = function(x, a, b) {
  unname(a * x ^ b)
}
mod_nums = lapply(meta[1:7], function(m) m$model)

mod_coefs = lapply(mod_nums, function(m) unlist(mod_key[mod_key$model == m,c("a_coef", "b_coef")]))

f800 = unlist(lapply(mod_coefs, function(m) power_f(800, m["a_coef"], m["b_coef"])))

alpha_m = unlist(lapply(post_list[1:7], function(m) post_summ(m, "alpha")[3,]))
alpha_l1 = unlist(lapply(post_list[1:7], function(m) post_summ(m, "alpha", probs = 0.025)[3,]))
alpha_l2 = unlist(lapply(post_list[1:7], function(m) post_summ(m, "alpha", probs = 0.1)[3,]))
alpha_u1 = unlist(lapply(post_list[1:7], function(m) post_summ(m, "alpha", probs = 0.975)[3,]))
alpha_u2 = unlist(lapply(post_list[1:7], function(m) post_summ(m, "alpha", probs = 0.75)[3,]))

par(mar = c(3,3,1,1), tcl = -0.25, mgp = c(2,0.5,0))
plot(alpha_m ~ seq(1,7), xaxt = "n", xlab = "Fecundity Data set", type = "n", ylim = range(alpha_l1, alpha_u1), ylab = "alpha")
segments(1:7, alpha_l1, 1:7, alpha_u1, lwd = c(2,1,1,1,1,1,1))
segments(1:7, alpha_l2, 1:7, alpha_u2, lwd = c(4,3,3,3,3,3,3))
points(alpha_m ~ seq(1,7), pch = 22, bg = "white", cex = 3, lwd = c(2,1,1,1,1,1,1))
text(alpha_m ~ seq(1,7), labels = letters[1:7], font = c(2,1,1,1,1,1,1))

```

##### (b)

```{r}
par(mar = c(3,3,1,1), tcl = -0.25, mgp = c(2,0.5,0))
plot(alpha_m ~ f800, type = "n", ylim = range(alpha_l1, alpha_u1), ylab = "alpha", xlab = "Expected Fecundity of 800 mm METF Female")
segments(f800, alpha_l1, f800, alpha_u1, lwd = c(2,1,1,1,1,1,1))
segments(f800, alpha_l2, f800, alpha_u2, lwd = c(4,3,3,3,3,3,3))
points(alpha_m ~ f800, pch = 22, bg = "white", cex = 3, lwd = c(2,1,1,1,1,1,1))
text(alpha_m ~ f800, labels = letters[1:7], font = c(2,1,1,1,1,1,1))
```

#### $S_{\mathrm{MSC}}$ {.tabset .tabset-pills .tabset-fade}

**Figure 2.3:** Consistency in the estimated value of $S_{\mathrm{MSC}}$ across assumed fecundity data sources and models for (a) 8-inch mesh, (b) 6-inch mesh, and (c) non-selective mesh. Points represent posterior medians and error bars represent the central 50% (thick lines) and 80% (thin lines) posterior credible regions.

##### (a) 8-inch

```{r S-msy-8-2, fig.width = 7, fig.height = 4}
x = msy["50%","S","mesh8",,]
upr1 = msy["90%","S","mesh8",,]
lwr1 = msy["10%","S","mesh8",,]
upr2 = msy["75%","S","mesh8",,]
lwr2 = msy["25%","S","mesh8",,]

col_mat = cbind(
  matrix(unique(col)[1], 3,4),
  matrix(unique(col)[2], 3,4)
)

zero_mods = str_detect(ids, "0")
ASL_mods = str_detect(ids, "ASL")

### 0-*
par(mfrow = c(1,2), oma = c(2,2,1,2), mar = c(1,1,1,1), mgp = c(2,0.5,0), tcl = -0.25)
mp = barplot(x[,zero_mods], beside = T, names.arg = letters[1:7],
             yaxt = "n",
             ylim = range(msy[c("10%", "90%"),"S",,,]) * c(0.95,1.05), col = "white", border = "white", xlab = "Fecundity Data set", ylab = "SMSC")
segments(mp, lwr1[,zero_mods], mp, upr1[,zero_mods], col = col[zero_mods])
segments(mp, lwr2[,zero_mods], mp, upr2[,zero_mods], col = col[zero_mods], lwd = 3)
points(mp, x[,zero_mods], col = "white", bg = col[zero_mods], pch = c(21, 22, 24), cex = 2)
box()
axis(side = 1, at = mp[2,], labels = F)
axis(side = 2, at = seq(0, 200000, 20000), labels = seq(0, 200, 20), las = 2)
legend("bottom", legend = c("First 10", "All", "Last 10"), 
       title = "Time Period", horiz = T, bty = "n", pch = c(21, 22, 24), col = "black", pt.bg = "grey80",
       pt.cex = 1.5)

mtext(side = 1, "Fecundity Data Set", line = 1.75)
mtext(side = 3, "No Trends (Model E-0)", line = 0.25, font = 2)


### ASL-
mp = barplot(x[,ASL_mods], beside = T, names.arg = letters[1:7],
             yaxt = "n",
             ylim = range(msy[c("10%", "90%"),"S",,,]) * c(0.95,1.05), col = "white", border = "white", xlab = "Fecundity Data set", ylab = "SMSC")
segments(mp, lwr1[,ASL_mods], mp, upr1[,ASL_mods], col = col[ASL_mods])
segments(mp, lwr2[,ASL_mods], mp, upr2[,ASL_mods], col = col[ASL_mods], lwd = 3)
points(mp, x[,ASL_mods], col = "white", bg = col[ASL_mods], pch = c(21, 22, 24), cex = 2)
box()
axis(side = 1, at = mp[2,], labels = F)
axis(side = 4, at = seq(0, 200000, 20000), labels = seq(0, 200, 20), las = 2)
mtext(side = 1, "Fecundity Data Set", line = 1.75)
mtext(side = 3, "All Trends (Model E-ASL)", line = 0.25, font = 2)


```

##### (b) 6-inch

```{r S-msy-6-2, fig.width = 7, fig.height = 4}
x = msy["50%","S","mesh6",,]
upr1 = msy["90%","S","mesh6",,]
lwr1 = msy["10%","S","mesh6",,]
upr2 = msy["75%","S","mesh6",,]
lwr2 = msy["25%","S","mesh6",,]

col_mat = cbind(
  matrix(unique(col)[1], 3,4),
  matrix(unique(col)[2], 3,4)
)

zero_mods = str_detect(ids, "0")
ASL_mods = str_detect(ids, "ASL")

### 0-*
par(mfrow = c(1,2), oma = c(2,2,1,2), mar = c(1,1,1,1), mgp = c(2,0.5,0), tcl = -0.25)
mp = barplot(x[,zero_mods], beside = T, names.arg = letters[1:7],
             yaxt = "n",
             ylim = range(msy[c("10%", "90%"),"S",,,]) * c(0.95,1.05), col = "white", border = "white", xlab = "Fecundity Data set", ylab = "SMSC")
segments(mp, lwr1[,zero_mods], mp, upr1[,zero_mods], col = col[zero_mods])
segments(mp, lwr2[,zero_mods], mp, upr2[,zero_mods], col = col[zero_mods], lwd = 3)
points(mp, x[,zero_mods], col = "white", bg = col[zero_mods], pch = c(21, 22, 24), cex = 2)
box()
axis(side = 1, at = mp[2,], labels = F)
axis(side = 2, at = seq(0, 200000, 20000), labels = seq(0, 200, 20), las = 2)
legend("top", legend = c("First 10", "All", "Last 10"), 
       title = "Time Period", horiz = T, bty = "n", pch = c(21, 22, 24), col = "white", pt.bg = "grey80",
       pt.cex = 1.5)

mtext(side = 1, "Fecundity Data Set", line = 1.75)
mtext(side = 3, "No Trends (Model E-0)", line = 0.25, font = 2)


### ASL-
mp = barplot(x[,ASL_mods], beside = T, names.arg = letters[1:7],
             yaxt = "n",
             ylim = range(msy[c("10%", "90%"),"S",,,]) * c(0.95,1.05), col = "white", border = "white", xlab = "Fecundity Data set", ylab = "SMSC")
segments(mp, lwr1[,ASL_mods], mp, upr1[,ASL_mods], col = col[ASL_mods])
segments(mp, lwr2[,ASL_mods], mp, upr2[,ASL_mods], col = col[ASL_mods], lwd = 3)
points(mp, x[,ASL_mods], col = "white", bg = col[ASL_mods], pch = c(21, 22, 24), cex = 2)
box()
axis(side = 1, at = mp[2,], labels = F)
axis(side = 4, at = seq(0, 200000, 20000), labels = seq(0, 200, 20), las = 2)
mtext(side = 1, "Fecundity Data Set", line = 1.75)
mtext(side = 3, "All Trends (Model E-ASL)", line = 0.25, font = 2)
```

##### (c) Flat

```{r S-msy-flat-2, fig.width = 7, fig.height = 4}
x = msy["50%","S","flat",,]
upr1 = msy["90%","S","flat",,]
lwr1 = msy["10%","S","flat",,]
upr2 = msy["75%","S","flat",,]
lwr2 = msy["25%","S","flat",,]

col_mat = cbind(
  matrix(unique(col)[1], 3,4),
  matrix(unique(col)[2], 3,4)
)

zero_mods = str_detect(ids, "0")
ASL_mods = str_detect(ids, "ASL")

### 0-*
par(mfrow = c(1,2), oma = c(2,2,1,2), mar = c(1,1,1,1), mgp = c(2,0.5,0), tcl = -0.25)
mp = barplot(x[,zero_mods], beside = T, names.arg = letters[1:7],
             yaxt = "n",
             ylim = range(msy[c("10%", "90%"),"S",,,]) * c(0.95,1.05), col = "white", border = "white", xlab = "Fecundity Data set", ylab = "SMSC")
segments(mp, lwr1[,zero_mods], mp, upr1[,zero_mods], col = col[zero_mods])
segments(mp, lwr2[,zero_mods], mp, upr2[,zero_mods], col = col[zero_mods], lwd = 3)
points(mp, x[,zero_mods], col = "white", bg = col[zero_mods], pch = c(21, 22, 24), cex = 2)
box()
axis(side = 1, at = mp[2,], labels = F)
axis(side = 2, at = seq(0, 200000, 20000), labels = seq(0, 200, 20), las = 2)
legend("top", legend = c("First 10", "All", "Last 10"), 
       title = "Time Period", horiz = T, bty = "n", pch = c(21, 22, 24), col = "white", pt.bg = "grey80",
       pt.cex = 1.5)

mtext(side = 1, "Fecundity Data Set", line = 1.75)
mtext(side = 3, "No Trends (Model E-0)", line = 0.25, font = 2)


### ASL-
mp = barplot(x[,ASL_mods], beside = T, names.arg = letters[1:7],
             yaxt = "n",
             ylim = range(msy[c("10%", "90%"),"S",,,]) * c(0.95,1.05), col = "white", border = "white", xlab = "Fecundity Data set", ylab = "SMSC")
segments(mp, lwr1[,ASL_mods], mp, upr1[,ASL_mods], col = col[ASL_mods])
segments(mp, lwr2[,ASL_mods], mp, upr2[,ASL_mods], col = col[ASL_mods], lwd = 3)
points(mp, x[,ASL_mods], col = "white", bg = col[ASL_mods], pch = c(21, 22, 24), cex = 2)
box()
axis(side = 1, at = mp[2,], labels = F)
axis(side = 4, at = seq(0, 200000, 20000), labels = seq(0, 200, 20), las = 2)
mtext(side = 1, "Fecundity Data Set", line = 1.75)
mtext(side = 3, "All Trends (Model E-ASL)", line = 0.25, font = 2)
```

# Section #3: Map for Year Indices

The indexing of brood and calendar years in the state-space model is somewhat counter-intuitive, thus we have included this section as an additional explanation.

The indexing of brood and calendar years is lagged by $a_{\mathrm{max}}+1$ years. For example, in our model,  $a_{\mathrm{max}}$ is 7, so recruits from the eighth tracked brood year are progeny of the spawners returning in the first tracked calendar year. The reason this lag is necessary is to populate the early years of calendar year abundance with expected adult returns so they can be used in the process model to produce the next recruits and to be fitted to the observations made in these early years. The extent of the lag is dependent on the maximum age of return ($a_{\mathrm{max}}$) because adults of each possible age must be populated in the first calendar year. The table below illustrates these concepts visually. Grey regions specify years that the model does not consider.

The calendar years ($t$) start counting at the year of first monitoring (1976 for our data set). The run that returned in this year was made up of age-4 fish from brood year 1972 (the fourth tracked brood year $y=4$), age-5 fish from brood year 1971 ($y=3$), age-6 fish from brood year 1970 ($y=2$), and age-7 fish from brood year 1969 ($y=1$) -- this illustrates why $a_{\mathrm{max}}$ brood years prior to data collection are necessary. 

The general expression for finding the brood year index $y$ in which fish returning in year $t$ at the $a^{\mathrm{th}}$ age were spawned is $y=t+n_a-a$, where $n_a$ is the number of ages of return and $a$ represents the index of each age of return (e.g., $a = 1$ for age-4). Our model tracked ages 4, 5, 6, and 7 returns for both sexes, so this calculation was the same for both males and females.

The model tracks recruitment through brood year 2015 only. This is because brood years later than this have not been observed in the data (for brood year 2016, the first possible age of return, age-4, returned in 2020, which is not in our data set). Thus, in order to inform the model expectation to link to observed return abundance and its age/sex structure in 2019 (the last observed calendar year), the age-4 fish from brood year 2015 are necessary but no more are necessary. Brood years 1969 ($y=1$) and 2015 ($y=47$) are the most poorly informed by data, as they are the most partially observed of any brood years (only age-7 and age-4, respectively). The information to estimate the recruitment states in these years comes from the hierarchical sharing of information in the probabilities of return-by-age and return-by-sex with the other brood years.

**Figure 3.1** Visual representation of the apportionment of brood year recruits to calendar year returns at age. Adapted from Figure 1 in @fleischman-etal-2013. Grey regions show years the model does not consider.

```{r, eval = T}
knitr::include_graphics(path = file.path(getwd(), "3-post-process/os-sensitivity/years-table/table.png"))
```

# Section #4: Alternative Composition Data Weighting

```{r workspace4}
model = 1  # just needed to build the data
source("2-model-fit/1-compile-data.R")
source("load-functions.R")
rm(model) # clear out the model object 

out_dir = "model-output/permanent"
out_files = dir(out_dir, full.names = T)

# select the models to read in models

# main text E-ASL model
keep_mods = 10

# altered age comp data weighting model
altered_ess_mods = mod_key$model[mod_key$supplementary == 1]

keep_mods = c(keep_mods, altered_ess_mods)

keep_mods = paste(paste0("-", keep_mods, "\\."), collapse = "|")
out_files = out_files[str_detect(out_files, keep_mods)]

# the file names of the output files
postfiles = out_files[str_detect(out_files, "post")]
mods = str_extract(postfiles, "[0-9]+")
metafiles = out_files[str_detect(out_files, "meta")]
msyfiles = out_files[str_detect(out_files, "msy")]

# create empty objects to store the output from each model
meta = list()
post_list = list()
msy = NULL

# read in the posterior samples and meta data
for (i in 1:length(mods)) {
  post_list[[i]] = readRDS(postfiles[i])
  meta[[i]] = readRDS(metafiles[i])
}

# create the ids for each model
ids = unlist(lapply(meta, id_model, unit = F, trends = F, src = F, rand_age = F, ess = T))

# read in the msy equilibrium quantities
msy = readRDS(msyfiles[1])
for (i in 2:length(mods)) {
  msy = abind(msy, readRDS(msyfiles[i]), along = 5)
}

# give the objects model identifiers
names(meta) = ids
names(post_list) = ids
dimnames(msy)[[5]] = ids
```

```{r model-identifers}
col = character(length(ids))
col[str_detect(ids, "s")] = "skyblue"
col[str_detect(ids, "m")] = "tomato"
lty = rep(1:4, 2)
lwd = c(3, rep(1, length(ids) - 1))
```

## Motivation and Methods

The use of multinomial likelihoods as a description of the observation process of age/sex composition by year and fate (escapement and fishery harvests) inserts a difficult problem related to the weight each data set receives. It is well-documented that the actual sample size (i.e., number of fish sampled) is not an appropriate measure of the amount of information contained in the sample because sampled fish are often not independent [@maunder-2011; @mcallister-ianelli-1997; @hulson-etal-2011]. This requires the use of an "effective multinomial sample size" (ESS), which is generally lower than the observed sample size. To address this issue for the analysis presented in the main text, we used a method where the effective sample size for each data set each year was rescaled such that the year with the maximum number of fish sampled for a data set received a ESS value of 100, and the rest of the years were scaled proportionately, i.e.:

$$ESS_{i,t} = \frac{n_{i,t}}{\mathrm{max}(n_{i})} \times 100$$  

where $i$ indexes the fate (escapement, commercial, or subsistence harvest) and $t$ indexes the calendar year, and $n$ is the number of fish sampled for age and sex composition. This method places more weight on years in which more fish were aged, however the value of 100 is arbitrary and places relatively equal weight on each of the fate-specific data sets. We refer to this scheme as the "scale" method.

We wished to evaluate whether the method we used to assign weight to these data sets is likely to have influenced our inference in the main text analysis. For this sensitivity analysis, we assessed the influence of placing more weight on some data sets over others by replacing the value of 100 with 200 for each data set sequentially (i.e., refitting the model with different weights for each data set).  Additionally, we assessed an alternative scheme where the value of $ESS_{i,t}$ was obtained as:

$$ESS_{i,t} = \mathrm{min}(n_{i,t}, X)$$

where $X$ took on the value of 100 or 200 depending on the scenario. We refer to this scheme as the "min" method, and it assumes that any additional fish beyond $X$ does not contribute any additional information to the multinomial sample. However, because the vast majority of years had greater than 200 fish for a given sampled fate, it has the tendency to weight all years equally within a data set. Table \@ref(tab:table2) below summarizes the data weighting scenarios investigated in this sensitivity analysis:

```{r table2}
tab = data.frame(
  scenario = 1:8,
  method = rep(c("scale", "min"), each = 4),
  esc = c(100, 200, 100, 100, 100, 200, 100, 100),
  com = c(100, 100, 200, 100, 100, 100, 200, 100),
  sub = c(100, 100, 100, 200, 100, 100, 100, 200),
  color = rep(c("blue", "red"), each = 4),
  lty = rep(c("solid", "dashed", "dotted", "dotdash"), 2),
  thick = c("thick", rep("thin", 7))
)

colnames(tab) = c("Scenario", "Method", "Escapement", "Commercial", "Subsistence", "Color", "Line Type<sup>b,c</sup>", "Line Thickness<sup>b</sup>")

tab$Scenario[1] = "1<sup>a</sup>"
tab %>%
  kable(align = "c", escape = F, caption = "Data weighting scenarios and their notation in the figures that follow.") %>%
  kable_styling(full_width = F, bootstrap_options = c("condensed")) %>%
  add_header_above(c(" " = 2, "Fate Data Set Scaler" = 3, "Symbology in Figures" = 3)) %>%
  footnote(alphabet = c(
    "This is the model E-ASL presented in the main text",
    "Only applicable in time series figures",
    "We recognize that it is difficult to identify the difference between these line types in the figures below, however that is part of the inference - the lines are similar so it does not matter which is which."))
```

All analyses were conducted using model structure E-ASL (egg count per female spawner as the reproductive unit, with time trends allowed for age-at-return, sex-at-return, and length-at-return).

### Results

As shown in the figures below, the influence of alternative composition weighting schemes was relatively minimal. There were several quantitative differences (estimates differed in their values), but rarely did the qualitative inference change (direction of trends, overall patterns). In general, we found:

*  Similar temporal declines in the probability of returning as a female regardless of age (Figure 4.1) and consistent agreement that the slope governing this pattern has at least 97.5% credibility of being negative (Figure 4.2b).
*  Similar temporal trends in the probability of returning at age by sex, with the exception of age 4 females -- models fitted using the "scale" method indicated slightly negative trends and models fitted with the "min" method indicated stronger positive trends, though both methods indicated age 4 fish make up 5% or less of all female recruits (Figure 4.3).
*  Similar calendar year age composition estimates, which reflect the fit to the available data (Figure 4.6a,b,c). 
*  Similar selectivity functions, though there was a tendency for the "min" models to have a less steep decline following peak selectivity as fish size increased (Figure 4.7).
*  Similar estimates of $S_{\mathrm{MSC}}$, as well as the effect of time period and mesh size (Figure 4.8a,b). However, there was a tendency for the "min" models to suggest $S_{\mathrm{MSC}}$ values that were approximately 10% larger than the "scale" models.
*  The use of the "min" weighting scheme resulted in narrower credible intervals for estimated and derived quantities -- this stems from the assumed greater information content of the data under these scheme relative to the "scale" method.

Although there are many other weighting schemes that we could have evaluated, we interpret these results as evidence that our choice of weighting scheme for the main analysis did not drive the results we obtained.

#### Return Composition {.tabset .tabset-pills .tabset-fade}

##### Sex

**Figure 4.1:** Variability in the probability of returning as a female across composition data weighting scenarios. Blue lines represent the "scale" ESS method, red lines represent the "min" method. The thick blue line is the scenario used in  main text analyses.

```{r sex-p, fig.width = 6, fig.height = 4}
by = (min(years) - a_max):(max(years) - a_min)
at_x = seq(1, length(by), 10)

par(mar = c(3,3,1,1), tcl = -0.25, mgp = c(2,0.45,0))
x = sapply(post_list, function(post) post_summ(post, "psi")[3,])
matplot(x, type = "l", ylim = c(0.25, 0.45), 
        lty = lty, col = col, lwd = lwd, xaxt = "n",
        xlab = "Brood Year", ylab = "Probability of Returning as Female")
axis(side = 1, at = at_x, labels = by[at_x])
```

**Figure 4.2:** Variability in (a) the intercept ($\delta_0$) and (b) the slope ($\delta_1$) of the logit-linear model governing the probability of returning as a female over time across composition data weighting scenarios. Points represent the posterior median and error bars represent the central 50% (thick lines) and 95% (thin lines) posterior credible regions. Blue represents weighting scenarios using the "scale" ESS method, red represents weighting scenarios using the "min" method.

```{r sex-coef, fig.width = 8, fig.height = 4}
par(mfrow = c(1,2), mar = c(0,1,1,1), oma = c(2.5,2,0,0), mgp = c(2,0.5,0), tcl = -0.25)
x = sapply(post_list, function(post) post_summ(post, "delta_0", probs = c(0.5, 0.25, 0.75, 0.025, 0.975)))
rownames(x) = c("mean", "sd", "50%", "25%", "75%", "2.5%", "97.5%")
mp = barplot(x["50%",], names.arg = 1:length(ids),
             ylim = range(0, x[c("2.5%", "97.5%"),]) * 1.05,
             col = "white", border = "white")
segments(mp, x["2.5%",], mp, x["97.5%",], col = col)
segments(mp, x["25%",], mp, x["75%",], lwd = 3, col = col)
points(mp, x["50%",], pch = 16, cex = 1.2, col = col)
axis(side = 1, at = mp, labels = F); box()
usr = par("usr"); xdiff = diff(usr[1:2]); ydiff = diff(usr[3:4])
text(usr[1], usr[4] - ydiff * 0.05, labels = "(a)", pos = 4, font = 2)

x = sapply(post_list, function(post) post_summ(post, "delta_1", probs = c(0.5, 0.25, 0.75, 0.025, 0.975)))
rownames(x) = c("mean", "sd", "50%", "25%", "75%", "2.5%", "97.5%")
mp = barplot(x["50%",], names.arg = 1:length(ids), ylim = range(0, x[c("2.5%", "97.5%"),]) * 1.05, col = "white", border = "white")
segments(mp, x["2.5%",], mp, x["97.5%",], col = col)
segments(mp, x["25%",], mp, x["75%",], lwd = 3, col = col)
points(mp, x["50%",], pch = 16, cex = 1.2, col = col)
axis(side = 1, at = mp, labels = F); box()
usr = par("usr"); xdiff = diff(usr[1:2]); ydiff = diff(usr[3:4])
text(usr[1], usr[4] - ydiff * 0.05, labels = "(b)", pos = 4, font = 2)

mtext(side = 1, outer = T, line = 1.5, "Data Weight Scenario")
mtext(side = 2, outer = T, line = 1, "Coefficient Value")
```

##### Age-by-Sex

**Figure 4.3:** Variability in the brood year-specific probability of returning at each age by sex across composition data weighting scenarios. Lines are posterior medians. Blue lines represent the weighting scenarios using the "scale" ESS method, red lines represent the weighting scenarios using the "min" method. The thick blue line is the scenario used in  main text analyses.

```{r age-p, fig.width = 5, fig.height = 7}
a = 1; s = 1
by = (min(years) - a_max):(max(years) - a_min)
at_x = seq(1, length(by), 10)
par(mfcol = c(4,2), mar = c(0.5,0.5,1.5,0.5), oma = c(3,4,0,2),
    tcl = -0.25, mgp = c(2,0.45,0))

sex = c("Female", "Male")
age = 4:7
for (s in 1:2) {
  for (a in 1:4) {
    p_sub = str_replace("pi[.+,a,s]", "a,s", paste0(a, ",", s))
    x = sapply(post_list, function(post) post_summ(post, p_sub)[3,])
    matplot(x, type = "l", lty = lty, col = col, lwd = lwd,
            ylim = c(0, max(max(x), 0.1)), xaxt = "n", yaxt = "n",
            main = paste(sex[s], "Age", age[a]))
    if (s == 1) axis(side = 2, las = 2) else axis(side = 4, las = 1)
    if (a == 4) {
      axis(side = 1, at = at_x, labels = by[at_x])
    } else {
      axis(side = 1, at = at_x, labels = F)
    }
  }
}
mtext(side = 1, outer = T, line = 1.5, "Brood Year")
mtext(side = 2, outer = T, line = 2, "Probability of Return-at-Age by Sex")

```

**Figure 4.4:** Variability in the intercepts ($\gamma_{0,a,s}$) of the baseline category logit-linear model governing the probability of returning at each age by sex over time across composition data weighting scenarios. Points represent the posterior median and error bars represent the central 50% (thick lines) and 95% (thin lines) posterior credible regions. Blue represents the weighting scenarios using the "scale" ESS method, red represents the weighting scenarios using the "min" method.

```{r age-coef-0, fig.width = 5, fig.height = 7}
par(mfcol = c(4,2), mar = c(0.5,0.5,1.5,0.5), oma = c(3,4,0,2),
    tcl = -0.25, mgp = c(2,0.45,0))

sex = c("Female", "Male")
age = 4:7
for (s in 1:2) {
  for (a in 1:4) {
    p_sub = str_replace("gamma_0[s,a]", "s,a", paste0(s, ",", a))
    x = sapply(post_list, function(post) post_summ(post, p_sub, probs = c(0.5, 0.25, 0.75, 0.025, 0.975)))
    rownames(x) = c("mean", "sd", "50%", "25%", "75%", "2.5%", "97.5%")
    mp = barplot(x["50%",], names.arg = 1:length(ids),
                 ylim = range(0, x[c("2.5%", "97.5%"),]) * 1.05,
                 col = "white", border = "white", xaxt = "n", yaxt = "n",
                 main = paste(sex[s], "Age", age[a]))
    segments(mp, x["2.5%",], mp, x["97.5%",], col = col)
    segments(mp, x["25%",], mp, x["75%",], lwd = 3, col = col)
    points(mp, x["50%",], pch = 16, cex = 1.2, col = col)
    if (s == 1) axis(side = 2, las = 2) else axis(side = 4, las = 1)
    if (a == 4) {
      axis(side = 1, at = mp, labels = 1:8)
    } else {
      axis(side = 1, at = mp, labels = F)
    }
    box()
  }
}

mtext(side = 1, outer = T, line = 1.5, "Data Weight Scenario")
mtext(side = 2, outer = T, line = 2, "Coefficient Value")


```

**Figure 4.5:** Variability in the slopes ($\gamma_{1,a,s}$) of the baseline category logit-linear model governing the probability of returning at each age by sex over time across composition data weighting scenarios. Points represent the posterior median and error bars represent the central 50% (thick lines) and 95% (thin lines) posterior credible regions. Blue represents the weighting scenarios using the "scale" ESS method, red represents the weighting scenarios using the "min" method.

```{r age-coef-1, fig.width = 5, fig.height = 7}
par(mfcol = c(4,2), mar = c(0.5,0.5,1.5,0.5), oma = c(3,4,0,2),
    tcl = -0.25, mgp = c(2,0.45,0))

sex = c("Female", "Male")
age = 4:7
for (s in 1:2) {
  for (a in 1:4) {
    p_sub = str_replace("gamma_1[s,a]", "s,a", paste0(s, ",", a))
    x = sapply(post_list, function(post) post_summ(post, p_sub, probs = c(0.5, 0.25, 0.75, 0.025, 0.975)))
    rownames(x) = c("mean", "sd", "50%", "25%", "75%", "2.5%", "97.5%")
    mp = barplot(x["50%",], names.arg = 1:length(ids),
                 ylim = range(0, x[c("2.5%", "97.5%"),]) * 1.05,
                 col = "white", border = "white", xaxt = "n", yaxt = "n",
                 main = paste(sex[s], "Age", age[a]))
    segments(mp, x["2.5%",], mp, x["97.5%",], col = col)
    segments(mp, x["25%",], mp, x["75%",], lwd = 3, col = col)
    points(mp, x["50%",], pch = 16, cex = 1.2, col = col)
    if (s == 1) axis(side = 2, las = 2) else axis(side = 4, las = 1)
    if (a == 4) {
      axis(side = 1, at = mp, labels = 1:8)
    } else {
      axis(side = 1, at = mp, labels = F)
    }
    box()
  }
}

mtext(side = 1, outer = T, line = 1.5, "Data Weight Scenario")
mtext(side = 2, outer = T, line = 2, "Coefficient Value")


```

#### Calendar Year Composition {.tabset .tabset-pills .tabset-fade}

**Figure 4.6:** Variability in the calendar year age/sex composition by fate (a - escapement; b - commercial; c - subsistence) across data weighting scenarios. Lines represent posterior medians. Blue lines represent the weighting scenarios using the "scale" ESS method, red lines represents the weighting scenarios using the "min" method. The thick blue line is the scenario used in main text analyses.

##### (a) Escapement

```{r q-esc, fig.width = 5, fig.height = 7}
type = "esc"
at_x = seq(1, nt, 10)
par(mfcol = c(4,2), mar = c(0.5,0.5,1.5,0.5), oma = c(3,4,0,2),
    tcl = -0.25, mgp = c(2,0.45,0))
sex = rep(c("Female", "Male"), each = 4)
age = rep(4:7, 2)
for (as in 1:8) {
    p_sub = str_replace("q_type[.+,as]", "as", as.character(as))
    p_sub = str_replace(p_sub, "type", type)
    x = sapply(post_list, function(post) post_summ(post, p_sub)[3,])
    matplot(x, type = "l", lty = lty, col = col, lwd = lwd,
            ylim = c(0, max(max(x), 0.05)), xaxt = "n", yaxt = "n",
            main = paste(sex[as], "Age", age[as]))
    if (as < 5) axis(side = 2, las = 2) else axis(side = 4, las = 1)
    if (as %in% c(4,8)) {
      axis(side = 1, at = at_x, labels = years[at_x])
    } else {
      axis(side = 1, at = at_x, labels = F)
    }
  }
mtext(side = 1, outer = T, line = 1.5, "Calendar Year")
mtext(side = 2, outer = T, line = 2, "Proportional Contribution")
```

##### (b) Commercial

```{r q-com, fig.width = 5, fig.height = 7}
type = "com"
at_x = seq(1, nt, 10)
par(mfcol = c(4,2), mar = c(0.5,0.5,1.5,0.5), oma = c(3,4,0,2),
    tcl = -0.25, mgp = c(2,0.45,0))
sex = rep(c("Female", "Male"), each = 4)
age = rep(4:7, 2)
for (as in 1:8) {
    p_sub = str_replace("q_type[.+,as]", "as", as.character(as))
    p_sub = str_replace(p_sub, "type", type)
    x = sapply(post_list, function(post) post_summ(post, p_sub)[3,])
    matplot(x, type = "l", lty = lty, col = col, lwd = lwd,
            ylim = c(0, max(max(x), 0.05)), xaxt = "n", yaxt = "n",
            main = paste(sex[as], "Age", age[as]))
    if (as < 5) axis(side = 2, las = 2) else axis(side = 4, las = 1)
    if (as %in% c(4,8)) {
      axis(side = 1, at = at_x, labels = years[at_x])
    } else {
      axis(side = 1, at = at_x, labels = F)
    }
  }
mtext(side = 1, outer = T, line = 1.5, "Calendar Year")
mtext(side = 2, outer = T, line = 2, "Proportional Contribution")
```

##### (c) Subsistence

```{r q-sub, fig.width = 5, fig.height = 7}
type = "sub"
at_x = seq(1, nt, 10)
par(mfcol = c(4,2), mar = c(0.5,0.5,1.5,0.5), oma = c(3,4,0,2),
    tcl = -0.25, mgp = c(2,0.45,0))
sex = rep(c("Female", "Male"), each = 4)
age = rep(4:7, 2)
for (as in 1:8) {
    p_sub = str_replace("q_type[.+,as]", "as", as.character(as))
    p_sub = str_replace(p_sub, "type", type)
    x = sapply(post_list, function(post) post_summ(post, p_sub)[3,])
    matplot(x, type = "l", lty = lty, col = col, lwd = lwd,
            ylim = c(0, max(max(x), 0.05)), xaxt = "n", yaxt = "n",
            main = paste(sex[as], "Age", age[as]))
    if (as < 5) axis(side = 2, las = 2) else axis(side = 4, las = 1)
    if (as %in% c(4,8)) {
      axis(side = 1, at = at_x, labels = years[at_x])
    } else {
      axis(side = 1, at = at_x, labels = F)
    }
  }
mtext(side = 1, outer = T, line = 1.5, "Calendar Year")
mtext(side = 2, outer = T, line = 2, "Proportional Contribution")
```

#### Selectivity Function

**Figure 4.7:** Variability in the estimated selectivity function across data weighting scenarios. Lines were obtained using the posterior medians of the selectivity parameters ($\tau$, $\sigma$, $\theta$, and $\lambda$). Blue lines represent the weighting scenarios using the "scale" ESS method, red lines represents the weighting scenarios using the "min" method. The thick blue line is the scenario used in  main text analyses.

```{r sel, fig.width = 6, fig.height = 4}
x = t(sapply(post_list, function(post) post_summ(post, "^V...$")[3,]))

pearson = function(rlm, params) {
  
  tau = params["Vtau"]
  sigma = params["Vsig"]
  theta = params["Vtha"]
  lambda = params["Vlam"]
  
  # separate calculation into 5 steps
  t1 = (1 + lambda^2/(4 * theta^2))^theta
  t2 = rlm - (sigma * lambda)/(2 * theta) - tau
  t3 = (1 + t2^2/sigma^2)^-theta
  t4 = exp(-lambda * (atan(t2/sigma) + atan(lambda/(2 * theta))))
  
  v = t1 * t3 * t4
  
  # standardize the output so only one age/sex is fully vuln for a gear
  v/max(v)
}

rlm_seq = seq(1.25, 3.25, length = 100)

out = apply(x, 1, function(z) pearson(rlm_seq, z))
par(mar = c(3,3,1,1), mgp = c(2,0.5,0), tcl = -0.25)
matplot(x = rlm_seq, y = out, type = "l", col = col, lwd = lwd, lty = lty,
        xlab = "Ratio of Fish Length to Mesh Perimeter (RLM)",
        ylab = "Selectivity", las = 1)


```


#### $S_{\mathrm{MSC}}$ {.tabset .tabset-pills .tabset-fade}

**Figure 4.8:** Variability in the estimated value of $S_{\mathrm{MSC}}$ across data weighting scenarios for (a) 8-inch mesh, (b) 6-inch mesh, and (c) non-selective mesh. Points represent posterior medians and error bars represent the central 50% (thick lines) and 80% (thin lines) posterior credible regions. Blue represents the weighting scenarios using the "scale" ESS method, red represent the weighting scenarios using the "min" method. 

##### (a) 8-inch

```{r S-msy-8, fig.width = 7, fig.height = 4}
x = msy["50%","S","mesh8",,]
upr1 = msy["90%","S","mesh8",,]
lwr1 = msy["10%","S","mesh8",,]
upr2 = msy["75%","S","mesh8",,]
lwr2 = msy["25%","S","mesh8",,]

col_mat = cbind(
  matrix(unique(col)[1], 3,4),
  matrix(unique(col)[2], 3,4)
)

par(mar = c(3,3,1,1), mgp = c(2,0.5,0), tcl = -0.25)
mp = barplot(x, beside = T, names.arg = 1:8, yaxt = "n",
             ylim = range(msy[c("10%", "90%"),"S",,,]) * c(0.95,1.05), col = "white", border = "white", xlab = "Data Weight Scenario", ylab = "SMSC")
segments(mp, lwr1, mp, upr1, col = col_mat)
segments(mp, lwr2, mp, upr2, col = col_mat, lwd = 3)
points(mp, x, col = "white", bg = col_mat, pch = c(21, 22, 24), cex = 1.5)
box()
axis(side = 1, at = mp[2,], labels = F)
axis(side = 2, at = seq(0, 200000, 20000), labels = seq(0, 200, 20), las = 2)
legend("bottomright", legend = c("First 10", "All", "Last 10"), 
       title = "Time Period", bty = "n", pch = c(21, 22, 24), col = "white", pt.bg = "grey80",
       pt.cex = 1.5)

```

##### (b) 6-inch

```{r S-msy-6, fig.width = 7, fig.height = 4}
x = msy["50%","S","mesh6",,]
upr1 = msy["90%","S","mesh6",,]
lwr1 = msy["10%","S","mesh6",,]
upr2 = msy["75%","S","mesh6",,]
lwr2 = msy["25%","S","mesh6",,]

col_mat = cbind(
  matrix(unique(col)[1], 3,4),
  matrix(unique(col)[2], 3,4)
)

par(mar = c(3,3,1,1), mgp = c(2,0.5,0), tcl = -0.25)
mp = barplot(x, beside = T, names.arg = 1:8, yaxt = "n",
             ylim = range(msy[c("10%", "90%"),"S",,,]) * c(0.95,1.05), col = "white", border = "white", xlab = "Data Weight Scenario", ylab = "SMSC")
segments(mp, lwr1, mp, upr1, col = col_mat)
segments(mp, lwr2, mp, upr2, col = col_mat, lwd = 3)
points(mp, x, col = "white", bg = col_mat, pch = c(21, 22, 24), cex = 1.5)
box()
axis(side = 1, at = mp[2,], labels = F)
axis(side = 2, at = seq(0, 200000, 20000), labels = seq(0, 200, 20), las = 2)
legend("topleft", legend = c("First 10", "All", "Last 10"), 
       title = "Time Period", bty = "n", pch = c(21, 22, 24), col = "white", pt.bg = "grey80",
       pt.cex = 1.5)
```

##### (c) Flat

```{r S-msy-flat, fig.width = 7, fig.height = 4}
x = msy["50%","S","flat",,]
upr1 = msy["90%","S","flat",,]
lwr1 = msy["10%","S","flat",,]
upr2 = msy["75%","S","flat",,]
lwr2 = msy["25%","S","flat",,]

col_mat = cbind(
  matrix(unique(col)[1], 3,4),
  matrix(unique(col)[2], 3,4)
)

par(mar = c(3,3,1,1), mgp = c(2,0.5,0), tcl = -0.25)
mp = barplot(x, beside = T, names.arg = 1:8, yaxt = "n",
             ylim = range(msy[c("10%", "90%"),"S",,,]) * c(0.95,1.05), col = "white", border = "white", xlab = "Data Weight Scenario", ylab = "SMSC")
segments(mp, lwr1, mp, upr1, col = col_mat)
segments(mp, lwr2, mp, upr2, col = col_mat, lwd = 3)
points(mp, x, col = "white", bg = col_mat, pch = c(21, 22, 24), cex = 1.5)
box()
axis(side = 1, at = mp[2,], labels = F)
axis(side = 2, at = seq(0, 200000, 20000), labels = seq(0, 200, 20), las = 2)
legend("topleft", legend = c("First 10", "All", "Last 10"), 
       title = "Time Period", bty = "n", pch = c(21, 22, 24), col = "white", pt.bg = "grey80",
       pt.cex = 1.5)
```

# Section #5: MCMC Details, Convergence, and Model Adequacy

## MCMC Attributes

MCMC sampling was conducted using long chains (100,000 burn-in samples, 1,000,000 samples following burn-in, thinning by 200 iterations, per each of 4 chains). This retained 20,000 samples for inference per fitted model.

## MCMC Convergence

We assessed MCMC simulation convergence using visual inspection of MCMC sampling behavior (i.e., trace plots) and the $\hat{R}$ diagnostic proposed by @brooks-gelman-1998. We also examined the effective Monte Carlo sample size, which provides an estimate of the number of independent MCMC samples obtained from each marginal posterior.

MCMC sampling behavior for these models was good under the specified dimensions, considering the complexity of this estimation problem (Supplements B -- I show summaries and diagnostic plots from select models). The only parameter that showed $\hat{R}$ values greater than 1.05 was $\alpha$ for models E-0, E-A, E-S, and E-AL (average $\hat{R}$ of $\alpha$ across these four models: 1.09). Given the rarity of these occurances (approximately 150 other estimated parameters, including 47 recruitment states and 88 fishing mortality parameters, across which the average maximum $\hat{R}$ value was 1.008), we were not concerned by this finding and proceeded with inference. The coefficients of the return-at-age model ($\gamma_{0,a,s}$ and $\gamma_{1,a,s}$) and two of the selectivity parameters ($\sigma$ and $\theta$) showed the poorest mixing across models, as evidenced by low effective MCMC samples relative to the other model parameters.

## Model Adequacy for the Data {.tabset .tabset-pills .tabset-fade}

```{r workspace5}
model = 1  # just needed to build the data
source("2-model-fit/1-compile-data.R")
source("load-functions.R")
rm(model) # clear out the model object 

out_dir = "model-output/permanent"
out_files = dir(out_dir, full.names = T)

# main text E-0 and E-ASL models
keep_mods = 1:12

keep_mods = paste(paste0("-", keep_mods, "\\."), collapse = "|")
out_files = out_files[str_detect(out_files, keep_mods)]

# the file names of the output files
postfiles = out_files[str_detect(out_files, "post")]
mods = str_extract(postfiles, "[0-9]+")
metafiles = out_files[str_detect(out_files, "meta")]

# create empty objects to store the output from each model
meta = list()
post_list = list()

# read in the posterior samples and meta data
for (i in 1:length(mods)) {
  # post_list[[i]] = post_thin(readRDS(postfiles[i]), keep_iters = 1000)
  post_list[[i]] = readRDS(postfiles[i])
  meta[[i]] = readRDS(metafiles[i])
}

# create the ids for each model
ids = unlist(lapply(meta, id_model, unit = T, trends = T))

# give the objects model identifiers
names(meta) = ids
names(post_list) = ids

# reorder the models
ordered_mods = c("N-0", "N-ASL", "E-0", "E-A", "E-S", "E-L", "E-AS", "E-AL", "E-SL", "E-ASL", "EM-0", "EM-ASL")
meta = meta[ordered_mods]
post_list = post_list[ordered_mods]
```

```{r bp-abundance}
f = function(post) {
  pp = lnorm_pp_check(post_subset(post, "^S_t[", T), S_obs_sig, S_obs)
  with(pp, mean(fit_obs > fit_new))
}
ESC_out = sapply(post_list, f)

f = function(post) {
  pp = lnorm_pp_check(post_subset(post, "^Hcom[", T), Hcom_obs_sig, Hcom_obs)
  with(pp, mean(fit_obs > fit_new))
}
COM_out = sapply(post_list, f)

f = function(post) {
  pp = lnorm_pp_check(post_subset(post, "^Hsub[", T), Hsub_obs_sig, Hsub_obs)
  with(pp, mean(fit_obs > fit_new))
}
SUB_out = sapply(post_list, f)

bp_N_tab = data.frame(Model = ordered_mods, Commercial = COM_out, Escapement = ESC_out, Subsistence = SUB_out)
rownames(bp_N_tab) = NULL
```

```{r bp-composition}
f = function(post) {
  pp = mult_pp_check(post_subset(post, "^q_esc[", T), x_esc, progress = F)
  with(pp, mean(fit_obs > fit_new))
}
esc_out = sapply(post_list, f)

f = function(post) {
  pp = mult_pp_check(post_subset(post, "^q_com[", T), x_com, progress = F)
  with(pp, mean(fit_obs > fit_new))
}
com_out = sapply(post_list, f)

f = function(post) {
  pp = mult_pp_check(post_subset(post, "^q_sub[", T), x_sub, progress = F)
  with(pp, mean(fit_obs > fit_new))
}
sub_out = sapply(post_list, f)

bp_comp_tab = data.frame(Model = ordered_mods, Commercial = com_out, Escapement = esc_out, Subsistence = sub_out)
rownames(bp_comp_tab) = NULL
```

We assessed model adequacy using posterior predictive checks [@kery-2010; @gelman-etal-2014], which compare the consistency of fit statistics between observed and simulated data following model assumptions. For each MCMC sample, we simulated hypothetical data sets from the posterior and calculated a fit criterion separately for the observed and simulated data relative to the model expectation for that iteration. We measured fit for the aggregate abundance data using the log-scale sum of squared error, i.e., 

$$SSQ_{k,i} = \sum_t^{n_t} \left(\log(x_{k,t,i}) - \log(\hat{x}_{k,t,i})\right)^2$$
where $x_{k,t}$ represents the abundance data (simulated or observed) for fate $k$ in year $t$ and $\hat{x}_{k,t,i}$ represents the model expectation for MCMC iteration $i$. For $x_{k,t,i}$, the $i$ index applies only for simulated data as a different set was drawn for each iteration. We used the $\chi^2$ statistic to summarize the fit of observed composition data from fate $k$:

$$\chi_{k,i}^2 = \sum_t^{n_t} \sum_j^{n_s \cdot n_a} \frac{(x_{k,t,j,i} - \hat{x}_{k,t,j,i})^2}{\hat{x}_{k,t,j,i}}$$
where $x_{k,t,j}$ represents the data (simulated or observed; counts at age/sex $j$) for year $t$, and $\hat{x}_{k,t,j}$ represents the model expectation for MCMC iteration $i$.

For each model and data set, we summarized these fit criteria as the proportion of MCMC iterations in which the observed data fitted more poorly than the simulated data (i.e., a Bayesian $p$-value), with values approaching 0.5 indicating ideal model fit and values  closer to zero or one indicating under- or over-dispersion, respectively [@kery-2010; @gelman-etal-2014].

**Table 5.1**  Bayesian $p$-values by data type (a -- abundance vs. b -- composition) and fate (commercial harvest, escapement, or subsistence harvest). Values close to 0.5 indicate good agreement between model and data, values closer to zero indicate data are under-dispersed relative to the model, and values closer to one indicate data are over-dispersed relative to the model.

```{r}
N_mods = str_detect(ordered_mods, "^N-")
E_mods = str_detect(ordered_mods, "^E-")
EM_mods = str_detect(ordered_mods, "^EM-")
a_mods = str_detect(ordered_mods, "A")
s_mods = str_detect(ordered_mods, "S")
l_mods = str_detect(ordered_mods, "L")

print_range = function(bp, fate, mods = rep(TRUE, 12)) {
  x = bp[mods,fate]
  rx = range(x)
  frx = format(rx, digits = 3, drop0trailing = F)
  paste(frx, collapse = " -- ")
}
```


### (a) Abundance

For aggregate abundance data by fate, Bayesian $p$-values ranged `r print_range(bp_N_tab, "Commercial")` for commercial, `r print_range(bp_N_tab, "Escapement")` for escapement, and `r print_range(bp_N_tab, "Subsistence")` for subsistence. Of these, only the escapement value is of possible concern, and it indicates the observed escapement data showed lack of fit relative to simulated data in some years -- particularly in years with the highest escapements (Online Supplements B -- I; Figure 3a in each document). This may imply that the assumed known observation uncertainty was lower than it perhaps should have been in these years.

```{r}
kable(bp_N_tab, digits = 3, align = c("lccc")) %>%
  kable_styling(full_width = F, bootstrap_options = "condensed")
```

### (b) Composition

For composition data by fate, Bayesian $p$-values showed more variability among models: a range of `r print_range(bp_comp_tab, "Commercial")` for commercial, `r print_range(bp_comp_tab, "Escapement")` for escapement, and `r print_range(bp_comp_tab, "Subsistence")` for subsistence. For each composition data set, the models that included time-varying probabilities of return-at-age resulted in the Bayesian $p$-values being closer to the ideal value of 0.5. For these models only (models `r StatonMisc::list_out(ordered_mods[a_mods], final = "and")`), the ranges were `r print_range(bp_comp_tab, "Commercial", a_mods)` for commercial, `r print_range(bp_comp_tab, "Escapement", a_mods)` for escapement, and `r print_range(bp_comp_tab, "Subsistence", a_mods)` for subsistence. This finding suggests the time-trending probability of return-by-age helped improve model adequacy for explaining variability in the data.

```{r}
kable(bp_comp_tab, digits = 3, align = c("lccc")) %>%
  kable_styling(full_width = F, bootstrap_options = "condensed")
```

# Section #6: $S_{R_{\mathrm{MAX}}}$ instead of $S_{\mathrm{MSC}}$

Our findings in the main-text equilibrium analysis illustrate that $S_{\mathrm{MSC}}$ is sensitive to the specific demographic trends, time periods, and size-selectivity used to derive it. However, here we illustrate that we would have made similar conclusions had we used an alternative biological reference point: $S_{R_{\mathrm{MAX}}}$. $S_{R_{\mathrm{MAX}}}$ represents the total escapement (regardless of sex or age composition) necessary to maximize recruitment. The yield-per-recruit approach we employed is general enough as to allow maximizing another objective value -- for deriving $S_{\mathrm{MSC}}$ in the main-text analysis we set the algorithm to maximize total catch but by replacing this with total recruitment, we can find $S_{R_{\mathrm{MAX}}}$ instead. Figure 6.1 below shows the same figure as presented in the main-text Figure 7, but for $S_{R_{\mathrm{MAX}}}$ and $H_{R_{\mathrm{MAX}}}$ (harvest at maximum recruitment). Note the overall similarity between this figure and that for $S_{\mathrm{MSC}}$ as shown in Figure 7 of the main-text.

**Figure 6.1** Estimated equilibrium values of $S_{R_{\mathrm{MAX}}}$ (panels a, c, and e) and $H_{R_{\mathrm{MAX}}}$ (panels b, d, and f) using 8-inch (panels a and b), non-selective (panels c and d), and 6-inch (panels e and f) mesh gillnet hear for a subset of models. The time period (symbol type) represents which years were used to calculate the average demographic qualities used in the equilibrium calculations. The percent change from the posterior median for model N-0 is displayed on the secondary $y$-axis for reference. Points are posterior medians, thick lines are the posterior central 50% limits, and thin lines are the posterior central 80% limits.

```{r workspace6, fig.height = 5.5, fig.width = 7.2}
rm(list = ls(all = T))
model = 1  # just needed to build the data
source("2-model-fit/1-compile-data.R")
source("load-functions.R")
rm(model) # clear out the model object 

out_dir = "model-output/permanent"
out_files = dir(out_dir, full.names = T)

# reduce the set of models to only main-text models
keep_mods = c(1,3,11,10,12)
keep_mods = paste(paste0("-", keep_mods, "\\."), collapse = "|")
out_files = out_files[str_detect(out_files, keep_mods)]

# the file names of the output files
mods = unique(str_extract(out_files, "[0-9]+"))
metafiles = out_files[str_detect(out_files, "meta")]
Rmaxfiles = out_files[str_detect(out_files, "Rmax")]

# create empty objects to store the output from each model
meta = list()
# read in the posterior samples and meta data
for (i in 1:length(mods)) {
  meta[[i]] = readRDS(metafiles[i])
}

# create the ids for each model
ids = unlist(lapply(meta, id_model))

# read in the msy equilibrium quantities
Rmax = readRDS(Rmaxfiles[1])
for (i in 2:length(mods)) {
  Rmax = abind(Rmax, readRDS(Rmaxfiles[i]), along = 5)
}


# give the objects model identifiers
names(meta) = ids
dimnames(Rmax)[[5]] = ids

Rmax_plot = function(keep_val = "S", keep_vuln = "mesh8", keep_mods, xticklabs = F, legend = F, letter) {
  # extract the equilibrium info requested
  keep = Rmax[,keep_val,keep_vuln,,keep_mods]
  ylwr = ifelse(keep_val == "S", 40000, 75000)
  yupr = ifelse(keep_val == "S", 180000, 325000)
  
  # create the basic plot: dimensions and spacing
  mp = barplot(keep["50%",,keep_mods], yaxt = "n", xaxt = "n", col = "white", border = "white", las = 2,
               beside = T, 
               ylim = range(Rmax[c("10%", "90%"),keep_val,,,keep_mods]) * c(0.95, 1.1),
               xlim = c(0.5, 20.5) 
               # , ylim = c(ylwr, yupr)
  )
  
  # extract user coordinates
  usr = par("usr"); xdiff = diff(usr[1:2]); ydiff = diff(usr[3:4])
  
  # locations to draw vertical lines separating models
  at_v = (mp[1,1:(length(keep_mods) - 1)] + mp[3,2:length(keep_mods)])/2
  abline(v = at_v, col = "grey60")
  
  # locations of breaks: zero trends vs. one vs. two trends
  zero_break = 3; one_break = 6; two_break = 9
  
  # determine locations and labels of tick marks for y-axes
  at_y1 = axisTicks(usr[3:4], log = F, nint = 4)
  at_y1_2 = seq(0, 500000, 10000)
  lab_y1 = at_y1/1000
  
  lab_y2 = seq(50, 250, 50)
  at_y2 = keep["50%","all","N-0"] * (lab_y2/100)
  
  # draw horizontal line at no change from N-0 model
  abline(h = at_y2[lab_y2 == 100], lty = 2)
  
  # draw y-axes
  axis(side = 2, at = at_y1, labels = lab_y1, las = 2)
  axis(side = 2, at = at_y1_2, labels = F, tcl = -0.125)
  axis(side = 4, at = at_y2, labels = paste0(lab_y2 - 100, "%"), las = 2)
  
  # draw thick line to separate zero vs. three trend models
  abline(v = at_v[zero_break], lwd = 4, xpd = F, col = "grey60")
  
  # draw credible intervals: central 80%
  segments(mp[1,], keep["10%","early",keep_mods], mp[1,], keep["90%","early",keep_mods])
  segments(mp[2,], keep["10%","all",keep_mods], mp[2,], keep["90%","all",keep_mods])
  segments(mp[3,], keep["10%","late",keep_mods], mp[3,], keep["90%","late",keep_mods])
  
  # draw credible intervals: central 50%
  segments(mp[1,], keep["25%","early",keep_mods], mp[1,], keep["75%","early",keep_mods], lwd = 4)
  segments(mp[2,], keep["25%","all",keep_mods], mp[2,], keep["75%","all",keep_mods], lwd = 4)
  segments(mp[3,], keep["25%","late",keep_mods], mp[3,], keep["75%","late",keep_mods], lwd = 4)
  
  # draw medians
  points(keep["50%","early",keep_mods] ~ mp[1,], pch = 21, bg = "grey60", cex = 1.2)
  points(keep["50%","all",keep_mods] ~ mp[2,], pch = 22, bg = "grey60", cex = 1.2)
  points(keep["50%","late",keep_mods] ~ mp[3,], pch = 24, bg = "grey60", cex = 1.2)
  
  # add a legend if requested
  if (legend) {
    legend("topright", y.intersp = 0.75, legend = c("First 10 years", "All years", "Last 10 years"), title = "Demography",
           pch = c(21,22,24), pt.bg = "grey60", pt.cex = 1, cex = 0.7, bg = "white", box.col = "white")
  }
  
  # draw x-axis
  if (xticklabs) {
    axis(side = 1, at = mp[2,], labels = keep_mods, las = 2)
  } else {
    axis(side = 1, at = mp[2,], labels = F, las = 2)
  }
  
  # draw the label identifying each plot
  rect(usr[1], usr[4] - ydiff * 0.11, usr[1] + xdiff * 0.35, usr[4], border = "white", col = "white")
  text(x = usr[1] + xdiff * -0.025, y = usr[4] - ydiff * 0.065,
       labels = ifelse(keep_vuln == "mesh8", paste0("(", letter, ") 8 in. mesh"),
                       ifelse(keep_vuln == "flat", paste0("(", letter, ") No selectivity"), paste0("(", letter, ") 6 in. mesh"))), pos = 4, cex = 0.7, font = 2)
  
  # add a main title over top panel
  mtext(side = 3, ifelse(keep_vuln == "mesh8", TeX(paste0(keep_val, "_{R_{MAX}}")), ""), line = 0, font = 2)
  
  # draw a border
  box()
}

# which models to keep for plot
keep_mods = c(
  "N-0",
  "E-0", "EM-0",
  # "E-L", "E-A", "E-S",
  # "E-AL", "E-AS", "E-SL",
  "E-ASL", "EM-ASL")

# make the plot itself
par(mfcol = c(3,2), xaxs = "i", yaxs = "i", cex = 1, lend = "square", mar = c(0,1.75,0.75,2.25),
    oma = c(3.5,0.75,0.5,2), tcl = -0.25, mgp = c(2,0.35,0), cex.axis = 0.9)
Rmax_plot("S", "mesh8", keep_mods, legend = F, letter = "a")
Rmax_plot("S", "flat", keep_mods, legend = F, letter = "c")
Rmax_plot("S", "mesh6", keep_mods, legend = T, xticklabs = T, letter = "e"); par(mar = c(0,2.25,0.75,1.75))
Rmax_plot("H", "mesh8", keep_mods, legend = F, letter = "b")
Rmax_plot("H", "flat", keep_mods, legend = F, letter = "d")
Rmax_plot("H", "mesh6", keep_mods, xticklabs = T, letter = "f")
mtext(side = 2, outer = T, "Escapement or Harvest (1000s)", line = -0.2)
mtext(side = 4, outer = T, "% Change from Model N-0", line = 0.75)
```

# References
